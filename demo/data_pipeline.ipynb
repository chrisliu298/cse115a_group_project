{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"data_pipeline.ipynb","provenance":[],"authorship_tag":"ABX9TyNiPywapE7BRWCemEcOgxyj"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"-LCvZiV2GJMW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":832},"executionInfo":{"status":"ok","timestamp":1595548574792,"user_tz":420,"elapsed":11348,"user":{"displayName":"Chris Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPAdKCEuGOWN3GLCOFI6LSY_oMStsJBoIgno5CPl8=s64","userId":"18245379379241780097"}},"outputId":"466296a5-180d-447a-d919-15fdafeeba18"},"source":["!pip install nlp\n","!pip install transformers\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: nlp in /usr/local/lib/python3.6/dist-packages (0.3.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from nlp) (0.3.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from nlp) (4.41.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from nlp) (3.0.12)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from nlp) (0.7)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from nlp) (2.23.0)\n","Requirement already satisfied: pyarrow>=0.16.0 in /usr/local/lib/python3.6/dist-packages (from nlp) (0.17.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from nlp) (1.18.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (2.10)\n","Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n","\u001b[K     |████████████████████████████████| 778kB 5.3MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 27.1MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Collecting sentencepiece!=0.1.92\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 39.7MB/s \n","\u001b[?25hCollecting tokenizers==0.8.1.rc1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.0MB 40.7MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=d4ee6445dd8e63a169583088c51ee543e61daed156b7205648d3108c1a5c2471\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OAMCNx7-GN3J","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":155},"executionInfo":{"status":"ok","timestamp":1595548470039,"user_tz":420,"elapsed":13005,"user":{"displayName":"Chris Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPAdKCEuGOWN3GLCOFI6LSY_oMStsJBoIgno5CPl8=s64","userId":"18245379379241780097"}},"outputId":"359e89a4-f9cc-4ff2-d29d-49e7b79f7f97"},"source":["!git clone https://github.com/chrisliu298/tapt.git"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Cloning into 'tapt'...\n","remote: Enumerating objects: 338, done.\u001b[K\n","remote: Counting objects: 100% (338/338), done.\u001b[K\n","remote: Compressing objects: 100% (239/239), done.\u001b[K\n","remote: Total 785 (delta 177), reused 245 (delta 88), pack-reused 447\u001b[K\n","Receiving objects: 100% (785/785), 147.77 MiB | 21.54 MiB/s, done.\n","Resolving deltas: 100% (386/386), done.\n","Checking out files: 100% (67/67), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZLQyliHOGVgU","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595548477448,"user_tz":420,"elapsed":343,"user":{"displayName":"Chris Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPAdKCEuGOWN3GLCOFI6LSY_oMStsJBoIgno5CPl8=s64","userId":"18245379379241780097"}}},"source":["import os\n","os.chdir(\"/content/tapt/src\")"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"cE3uf8YCGaZ9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":565},"executionInfo":{"status":"error","timestamp":1595548880746,"user_tz":420,"elapsed":29228,"user":{"displayName":"Chris Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPAdKCEuGOWN3GLCOFI6LSY_oMStsJBoIgno5CPl8=s64","userId":"18245379379241780097"}},"outputId":"626ac9f6-bc0f-4730-9250-b30d4aaea218"},"source":["import pandas as pd\n","from pprint import pprint\n","from transformers import AutoModelForSequenceClassification\n","from transformers import AutoTokenizer\n","\n","from utils.data_pipeline import prepare_data\n","from utils.data_pipeline import prepare_custom_data\n","from utils.metrics import compute_metrics\n","\n","\n","def tokenize(batch):\n","    \"\"\"Tokenize a batch of data (with padding and truncation).\n","\n","    Arg:\n","        batch: A batch of training data.\n","    \"\"\"\n","    return tokenizer(\n","        batch[\"text\"], padding=\"max_length\", truncation=True, max_length=512\n","    )\n","\n","# Load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","# Load model\n","model = AutoModelForSequenceClassification.from_pretrained(\"distilroberta-base\")\n","\n","# Load dataset\n","train_dataset, val_dataset, test_dataset = prepare_data(\n","    tokenize_func=tokenize,\n","    dataset_name=\"yelp_polarity\",\n","    train_count=10,\n","    train_size=5,\n","    val_size=5,\n","    use_all_test=False,\n","    test_count=10,\n","    test_size=5,\n","    others=5,\n","    seed=42,\n",")\n","# Load custom data\n","augmented = prepare_custom_data(\n","    tokenize_func=tokenize, dataset_name=\"/content/nlp_yelp_train.tsv\"\n",")"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","100%|██████████| 560000/560000 [00:14<00:00, 39520.22it/s]\n","100%|██████████| 5/5 [00:00<00:00, 7570.95it/s]\n","100%|██████████| 5/5 [00:00<00:00, 4616.23it/s]\n","100%|██████████| 1/1 [00:00<00:00, 112.81it/s]\n","100%|██████████| 1/1 [00:00<00:00, 77.56it/s]\n"],"name":"stderr"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-cc5a5c3f4ec4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Load custom data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m augmented = prepare_custom_data(\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mtokenize_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/nlp_yelp_train.tsv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m )\n","\u001b[0;32m/content/tapt/src/utils/data_pipeline.py\u001b[0m in \u001b[0;36mprepare_custom_data\u001b[0;34m(tokenize_func, dataset_name, slice)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: type object 'Dataset' has no attribute 'from_pandas'"]}]},{"cell_type":"code","metadata":{"id":"eeccGq1sGsXI","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}